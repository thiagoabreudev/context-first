---
name: context-first-test-first
description: Criar testes ANTES do código (TDD adaptado para IA)
tools: Read, Write, Glob, Grep, Bash
---

# Test-First Command

## Uso
```
/context-first test-first <feature_description>
```

## Exemplo
```
/context-first test-first "Criar endpoint POST /api/login que valida email/senha e retorna JWT"
```

<feature_description>
#$ARGUMENTS
</feature_description>

---

## Passo 0: Carregar Variáveis de Ambiente

Antes de executar, carregar a variável `METASPECS_DIR` do arquivo `.ia.env`:

```bash
# Carregar .ia.env
if [ -f .ia.env ]; then
    export $(grep -v '^#' .ia.env | xargs)
    echo "✅ METASPECS_DIR carregado: $METASPECS_DIR"
else
    echo "❌ Arquivo .ia.env não encontrado. Crie baseado em .ia.env.example"
    exit 1
fi
```

**Importante**: Todos os caminhos de metaspecs devem usar `$METASPECS_DIR` como prefixo.

---

## Comportamento

### Passo 1: Ler Metaspecs Relevantes

1. **Identificar tipo de feature** baseado em <feature_description>
   - Keywords: login, auth, api, endpoint, etc.

2. **Ler Metaspec Map**:
   - `$METASPECS_DIR/METASPEC_MAP.md`

3. **Identificar metaspecs obrigatórias** para o tipo

4. **Ler TODAS as metaspecs obrigatórias** de `$METASPECS_DIR`

---

### Passo 2: Identificar Behaviors

Baseado nas metaspecs, identificar:

#### Must Do (O que o código DEVE fazer)
- ✅ Validações obrigatórias
- ✅ Fluxos principais
- ✅ Regras de negócio críticas

#### Must Not Do (O que o código NÃO DEVE fazer)
- ❌ Anti-patterns
- ❌ Violações de segurança
- ❌ Comportamentos proibidos

#### Edge Cases (Casos extremos)
- ⚠️ Inputs inválidos
- ⚠️ Timeouts
- ⚠️ Erros de rede
- ⚠️ Dados faltando

---

### Passo 3: Criar Testes

Gerar arquivo de teste completo com:

1. **Imports necessários**
2. **Fixtures** (se necessário)
3. **Testes para Must Do** (casos de sucesso)
4. **Testes para Must Not Do** (validações de anti-patterns)
5. **Testes para Edge Cases** (casos de falha)

**Estrutura de teste**:

```python
# tests/test_<feature_slug>.py
# Generated by /context-first test-first
# Feature: <feature_description>
# Based on metaspecs:
#   - $METASPECS_DIR/business/<spec1>.md
#   - $METASPECS_DIR/technical/<spec2>.md

import pytest
from decimal import Decimal

class Test<FeatureName>:
    """Testes para: <feature_description>"""
    
    # ===== MUST DO: Casos de Sucesso =====
    
    def test_should_validate_all_inputs(self):
        """DEVE validar todos os inputs."""
        # Arrange
        # Act
        # Assert
        pass
    
    def test_should_return_expected_output(self):
        """DEVE retornar output esperado."""
        pass
    
    # ===== MUST NOT DO: Anti-Patterns =====
    
    def test_should_not_expose_sensitive_data(self):
        """NÃO DEVE expor dados sensíveis em logs ou responses."""
        pass
    
    def test_should_not_use_hardcoded_values(self):
        """NÃO DEVE usar valores hardcoded."""
        pass
    
    # ===== EDGE CASES: Casos de Falha =====
    
    def test_should_handle_invalid_input(self):
        """DEVE tratar input inválido."""
        pass
    
    def test_should_timeout_after_30s(self):
        """DEVE dar timeout após 30 segundos."""
        pass
    
    def test_should_handle_database_unavailable(self):
        """DEVE tratar banco de dados indisponível."""
        pass
```

---

### Passo 4: Documentar Testes Criados

Salvar arquivo de teste e informar ao usuário:

```
✅ Testes criados: tests/test_<feature_slug>.py

Testes gerados (15 total):
  ✅ Must Do (8 testes)
    - test_should_validate_all_inputs
    - test_should_return_expected_output
    - ...
  
  ❌ Must Not Do (4 testes)
    - test_should_not_expose_sensitive_data
    - test_should_not_use_hardcoded_values
    - ...
  
  ⚠️ Edge Cases (3 testes)
    - test_should_handle_invalid_input
    - test_should_timeout_after_30s
    - test_should_handle_database_unavailable

Baseado em metaspecs:
  - $METASPECS_DIR/business/authentication/login.md
  - $METASPECS_DIR/technical/api/security.md

Próximos passos:
  1. Revisar testes gerados
  2. Executar testes (devem FALHAR - TDD)
  3. Implementar código para passar nos testes
  4. Usar /engineer work para implementar
```

---

### Passo 5: Perguntar ao Usuário

Perguntar:
> "Revisei as metaspecs e criei 15 testes. Quer que eu:
> 1. Mostre os testes para revisão?
> 2. Execute os testes (devem falhar - TDD)?
> 3. Prossiga para implementação?"

Aguardar resposta antes de prosseguir.

---

## Exemplo de Output

```python
# tests/api/test_login.py
# Generated by /context-first test-first
# Feature: Criar endpoint POST /api/login que valida email/senha e retorna JWT
# Based on metaspecs:
#   - $METASPECS_DIR/business/authentication/login.md
#   - $METASPECS_DIR/technical/api/security.md

import pytest
from fastapi.testclient import TestClient
from src.main import app

client = TestClient(app)

class TestLoginEndpoint:
    """Testes para endpoint de login."""
    
    # ===== MUST DO: Casos de Sucesso =====
    
    def test_should_login_with_valid_credentials(self):
        """DEVE fazer login com credenciais válidas."""
        response = client.post('/api/login', json={
            'email': 'user@example.com',
            'password': 'Password123!'
        })
        assert response.status_code == 200
        assert 'access_token' in response.json()
        assert 'token_type' in response.json()
        assert response.json()['token_type'] == 'bearer'
    
    def test_should_validate_email_format(self):
        """DEVE validar formato de email."""
        response = client.post('/api/login', json={
            'email': 'invalid-email',
            'password': 'Password123!'
        })
        assert response.status_code == 400
        assert 'email' in response.json()['detail'].lower()
    
    def test_should_validate_password_presence(self):
        """DEVE validar presença de senha."""
        response = client.post('/api/login', json={
            'email': 'user@example.com',
            'password': ''
        })
        assert response.status_code == 400
        assert 'password' in response.json()['detail'].lower()
    
    # ===== MUST NOT DO: Anti-Patterns =====
    
    def test_should_not_expose_password_in_errors(self):
        """NÃO DEVE expor senha em mensagens de erro."""
        response = client.post('/api/login', json={
            'email': 'user@example.com',
            'password': 'WrongPassword123!'
        })
        assert response.status_code == 401
        # Verificar que senha NÃO está na resposta
        assert 'WrongPassword123!' not in response.text
        assert 'password' not in response.json().get('detail', '').lower()
    
    def test_should_not_expose_user_existence(self):
        """NÃO DEVE expor se usuário existe ou não."""
        # Usuário inexistente
        response1 = client.post('/api/login', json={
            'email': 'nonexistent@example.com',
            'password': 'Password123!'
        })
        
        # Usuário existente com senha errada
        response2 = client.post('/api/login', json={
            'email': 'user@example.com',
            'password': 'WrongPassword!'
        })
        
        # Mensagens devem ser genéricas e iguais
        assert response1.status_code == 401
        assert response2.status_code == 401
        assert response1.json()['detail'] == response2.json()['detail']
    
    def test_should_not_log_password(self):
        """NÃO DEVE logar senha em plain text."""
        # Este teste verifica que senha não aparece em logs
        # Implementação depende do sistema de logging
        pass
    
    # ===== EDGE CASES: Casos de Falha =====
    
    def test_should_handle_missing_fields(self):
        """DEVE tratar campos faltando."""
        response = client.post('/api/login', json={})
        assert response.status_code == 400
    
    def test_should_handle_extra_fields(self):
        """DEVE ignorar campos extras."""
        response = client.post('/api/login', json={
            'email': 'user@example.com',
            'password': 'Password123!',
            'extra_field': 'should_be_ignored'
        })
        # Não deve dar erro, apenas ignorar
        assert response.status_code in [200, 401]
    
    def test_should_timeout_after_30s(self):
        """DEVE dar timeout após 30 segundos."""
        # Simular operação lenta
        # Implementação depende do framework
        pass
    
    def test_should_handle_database_unavailable(self):
        """DEVE tratar banco de dados indisponível."""
        # Mock database connection failure
        # Implementação depende do framework
        pass
    
    def test_should_rate_limit_login_attempts(self):
        """DEVE limitar tentativas de login (rate limiting)."""
        # Fazer múltiplas tentativas
        for _ in range(10):
            client.post('/api/login', json={
                'email': 'user@example.com',
                'password': 'WrongPassword!'
            })
        
        # 11ª tentativa deve ser bloqueada
        response = client.post('/api/login', json={
            'email': 'user@example.com',
            'password': 'Password123!'
        })
        assert response.status_code == 429  # Too Many Requests
```

---

## Relacionado

- [/engineer work](../engineer/work.md) - Implementar código para passar nos testes
- [/context-first validate-spec](./validate-spec.md) - Validar código contra metaspecs
- [/metaspecs validate](../metaspecs/validate.md) - Validar metaspecs lidas
- [@branch-test-planner](../../agents/branch-test-planner.md) - Analisar cobertura de testes
